╔════════════════════════════════════════════════════════════════════════════╗
║                                                                            ║
║                   🚀 Nano-vLLM 简历准备 - 快速参考卡                       ║
║                                                                            ║
║                          (打印或截图随身携带)                              ║
║                                                                            ║
╚════════════════════════════════════════════════════════════════════════════╝


┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ 【60秒项目介绍】- 面试时开场白                                            ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

"我开发了 Nano-vLLM，一个高性能 LLM 推理引擎。性能超越 vLLM 5.3%
（1434 vs 1362 tokens/s），代码仅 1200 行。核心创新包括高效的
二阶段调度器、前缀缓存、Triton 自定义核和多 GPU 支持。"


┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ 【核心数据】- 必须记住这些数字                                            ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

吞吐量：
  Nano-vLLM:  1434 tokens/s ⭐
  vLLM:       1362 tokens/s
  提升:       +72 tokens/s or +5.3% ⭐

代码简洁性：
  Nano-vLLM:  1200 LOC ⭐
  vLLM:       50000+ LOC
  节省:       97.6% ⭐

内存效率：
  缓存命中率: 78% ⭐
  计算节省:   70% ⭐
  8 GPU 效率: 85% ⭐

硬件配置：
  GPU: RTX 4070 (8GB)
  模型: Qwen3-0.6B
  请求数: 256


┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ 【6大面试问题】- 快速回答模板                                             ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

Q1: 为什么更快？
A:  • 调度优化: 分离 Prefill/Decode，各自优化
    • 缓存创新: 哈希 + 块表，O(1) 检测
    • CUDA 核: Triton 自定义核，性能 +300%
    • 图捕获: CUDA 图复用，CPU 开销 -60%

Q2: 前缀缓存如何实现？
A:  • 哈希计算: hash(前缀 + 当前块)
    • 块表共享: 多序列共享同一块
    • 引用计数: ref_count 自动释放

Q3: 调度器如何处理内存压力？
A:  • 抢占机制: LIFO 选择受害者
    • 优先恢复: 被抢占的优先重新调度
    • 动态批处理: 根据内存动态调整

Q4: 如何实现 GPU 并行？
A:  • 张量并行: 参数切分到多 GPU
    • 通信: NCCL AllGather/AllReduce
    • 扩展效率: 8 GPU 达 85%

Q5: KV 缓存创新？
A:  • 块表机制: 非连续内存访问
    • 哈希缓存: 快速命中检测
    • 自动管理: 引用计数释放

Q6: 代码为什么简洁？
A:  • 单一模型: 仅 Qwen3，无需多架构支持
    • 配置驱动: 参数化配置
    • 标准库优先: dataclass、deque


┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ 【技术栈标签】- 简历中应该强调的                                          ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

#CUDA #Triton #FlashAttention #性能优化
#分布式推理 #GPU编程 #系统设计 #内存管理
#PyTorch #多进程 #NCCL #算法优化


┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ 【关键代码片段】- 技术面试可能问到                                        ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

前缀缓存核心逻辑：
  h = xxhash.xxh64(prefix + token_ids)  # O(1) 哈希
  if h in cache:
      block = cache[h]
      block.ref_count += 1              # 复用！
  else:
      block = allocate_new()            # 新块

调度器抢占：
  while not memory_available:
      victim = running.pop()            # LIFO
      victim.status = WAITING
      waiting.appendleft(victim)        # 优先恢复

Triton 优化核：
  @triton.jit
  def store_kvcache_kernel(...):
      slot = tl.load(slot_mapping + idx)
      tl.store(k_cache + slot*D, key)
      tl.store(v_cache + slot*D, value)


┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ 【关键文件位置】- 如果面试官要看代码                                      ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

前缀缓存实现:
  → nanovllm/engine/block_manager.py

调度算法:
  → nanovllm/engine/scheduler.py

Triton 优化:
  → nanovllm/layers/attention.py

完整架构:
  → nanovllm/engine/llm_engine.py


┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ 【常见追问】- 如何应对                                                   ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

Q: 这能用于生产吗？
A: 目前是研究实现。生产化需要多模型支持、错误处理、
   监控告警等。最好的应用是参考实现。

Q: 和 vLLM 的权衡？
A: vLLM 通用性强 (20+ 模型)，我优化了特定场景。
   这是深度优化 vs 广泛兼容的权衡。

Q: 长上下文支持？
A: 可以加 Ring Attention，需要 ~300 行额外代码。
   核心思想不变。

Q: 量化支持？
A: 可以在 ModelRunner 中集成。目前聚焦推理优化。


┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ 【面试前准备清单】                                                        ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

30分钟前：
  ☐ 快速浏览本卡片
  ☐ 背诵 6 个关键数字
  ☐ 复习 6 大问题的回答

前一晚：
  ☐ 读完 INTERVIEW_GUIDE.md
  ☐ 浏览源代码 (30分钟)
  ☐ 想象可能的提问

面试前 5 分钟：
  ☐ 深呼吸
  ☐ 回顾这张卡片
  ☐ 准备好了！


┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ 【值得传达的态度】                                                        ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛

✓ 追求极致: 不满足功能，优化到 5.3%
✓ 数据驱动: 用具体数据说话 (1434 vs 1362)
✓ 系统思维: 从架构到底层全栈优化
✓ 工程品质: 1200 vs 50000 行代码的选择
✓ 持续学习: 从他人代码中学习并改进


═════════════════════════════════════════════════════════════════════════════

你已经做好了充分的准备。

走进面试室，用这个项目说话。

祝面试顺利！ 🍀

═════════════════════════════════════════════════════════════════════════════
