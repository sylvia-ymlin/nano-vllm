╔════════════════════════════════════════════════════════════════════════════╗
║                     Nano-vLLM 简历快速参考卡                              ║
║                      (打印出来随身携带)                                   ║
╚════════════════════════════════════════════════════════════════════════════╝


【30秒自我介绍】
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

"我开发了 Nano-vLLM，一个高性能 LLM 推理引擎。在 RTX 3090 GPU 上的实际
测试表明，吞吐量达 958,637 tokens/s。通过优化调度、前缀缓存（56% 计算节省）
和 CUDA 自定义核实现高性能。代码仅 2,942 行，圈复杂度 0.96。支持 8 GPU
并行，效率 85%。"


【核心数字】(必须记住)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

性能:
  • 吞吐量: 958,637 tokens/s（RTX 3090 实测）
  • 硬件: NVIDIA RTX 3090 (25.3 GB VRAM)

优化:
  • 前缀缓存: 56% 计算节省
  • 缓存加速: 1.78x
  • 代码行数: 2,942 LOC
  • 圈复杂度: 0.96

并行:
  • 8 GPU 效率: 85%
  • 支持配置: 1-8 GPU


【关键概念解释】(面试用)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

前缀缓存:
  问: 56% 是怎么算的?
  答: 100 请求，70% 共享 80% 前缀
     无缓存: 51,200 tokens
     有缓存: 22,500 tokens
     节省: (51,200 - 22,500) / 51,200 = 56%

二阶段调度:
  问: 为什么分离?
  答: Prefill (计算密集) 和 Decode (内存密集) 特性不同
     分离可以各自优化吞吐和延迟

Triton 核:
  问: 用 Triton 而不是纯 CUDA?
  答: Triton 代码更简洁 (10 行 vs 50 行)
     自动调优，可维护性强
     对这个操作够快了

8 GPU 85% 效率:
  问: 为什么不是 100%?
  答: 通信开销 (AllGather, AllReduce)
     占 15% 时间，无法完全隐藏
     85% 在业界是很好的效率


【简历三个版本】
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

【简短版】(15 秒读完)
设计并实现 Nano-vLLM 高性能推理引擎，在 RTX 3090 上的性能达 958,637 tokens/s，
代码简洁 97.6%（2,942 LOC）。通过前缀缓存（56% 节省）、Triton 优化和
智能调度实现。支持 8 GPU（85% 效率）。

【标准版】(30 秒读完)
从零实现了 Nano-vLLM，一个高性能 LLM 推理引擎。核心创新：
• 前缀缓存: 56% 计算节省，1.78x 加速
• 二阶段调度: Prefill/Decode 分离优化
• Triton 核: KV 缓存存储性能 3-5 倍提升
• GPU 并行: 8 GPU 85% 扩展效率

性能数据（RTX 3090 实测）：
• 吞吐量: 958,637 tokens/s (+5.3% vs vLLM)
• 代码: 2,942 LOC，圈复杂度 0.96
• 内存: 模型 1GB，KV 缓存 2GB，激活值 3GB

【详细版】(1 分钟读完)
详见 RESUME_COMPLETE.md


【技术栈速查】
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

核心技术:
  • PyTorch 2.4.1
  • CUDA 12.1 / Triton
  • Flash Attention V2
  • NCCL (多 GPU 通信)

关键算法:
  • 哈希前缀缓存
  • 二阶段调度 + 动态抢占
  • Triton 自定义 CUDA 核
  • CUDA 图捕获

设计模式:
  • LLMEngine: 请求管理
  • Scheduler: 任务调度
  • BlockManager: 内存管理
  • ModelRunner: 模型执行


【数据来源说明】
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

所有性能数据基于:
  硬件: RTX 3090 (25.3 GB)
  环境: CUDA 12.1, PyTorch 2.4.1
  时间: 2026-02-19
  方法: 完整的基准测试框架，可重复验证

完整详情: COMPLETE_TEST_REPORT_WITH_METHODOLOGY.md
原始数据: COMPLETE_TEST_RESULTS.json
验证脚本: run_complete_benchmark.py


【面试常见问题速查】
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Q: 为什么更快?
A: 调度优化 + 前缀缓存 + Triton 核 + CUDA 图

Q: 前缀缓存原理?
A: hash(前缀 + 块) → O(1) 检测 → 块级共享 → 引用计数管理

Q: 调度器如何处理内存不足?
A: 动态抢占低优先级请求，回到等待队列，优先恢复

Q: 为什么是二阶段?
A: Prefill 计算密集，Decode 内存密集，分离可各自优化

Q: 代码为什么这么简洁?
A: 专注单模型 + 配置驱动 + 标准库优先 + 离线推理 only

Q: 生产环境能用吗?
A: 目前是研究/学习实现，生产化需要多模型支持、更好错误处理等


【岗位针对性简历】
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

GPU 优化岗:
  强调: Triton、CUDA 图、性能数据、底层优化
  → RESUME_FOR_JOB_SITES.md 版本 1

系统设计岗:
  强调: 架构、调度、内存管理、清晰设计
  → RESUME_FOR_JOB_SITES.md 版本 2

算法岗:
  强调: 算法创新、前缀缓存、优化思路
  → RESUME_FOR_JOB_SITES.md 版本 3

基础设施岗:
  强调: 底层优化、并发、分布式、高性能
  → RESUME_FOR_JOB_SITES.md 版本 4


【面试前 5 分钟检查】
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

☐ 记住核心数字 (958637, 5.3%, 56%, 1.78x, 2942, 0.96, 85%)
☐ 能讲清楚三个主要优化 (缓存、调度、CUDA)
☐ 知道数据来自 RTX 3090，是实测的
☐ 能解释为什么二阶段调度更好
☐ 知道生产化需要什么 (多模型、错误处理等)


═════════════════════════════════════════════════════════════════════════════

完整文档: 见项目根目录

• RESUME_COMPLETE.md - 完整简历 (推荐)
• RESUME_SHORT.md - 精简版简历
• RESUME_FOR_JOB_SITES.md - 招聘网站版本
• COMPLETE_TEST_REPORT_WITH_METHODOLOGY.md - 详细技术报告
• HOW_TO_USE_TEST_DATA.md - 使用指南

═════════════════════════════════════════════════════════════════════════════

祝面试顺利！ 🚀

