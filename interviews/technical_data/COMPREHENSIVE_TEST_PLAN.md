# ğŸ”¬ Nano-vLLM å®Œæ•´æµ‹è¯•è®¡åˆ’

**åŸåˆ™**ï¼šæ‰€æœ‰æ•°æ®æ¥è‡ª**åŒä¸€å¥—ç¡¬ä»¶ç¯å¢ƒ**çš„å®Œæ•´æµ‹è¯•ï¼Œæ¯ä¸ªæ•°æ®éƒ½éœ€è¦è¯´æ˜æµ‹è¯•æ–¹æ³•å’Œè®¡ç®—æ–¹å¼ã€‚

---

## ğŸ“‹ æµ‹è¯•ç¯å¢ƒè§„æ ¼

### ç¡¬ä»¶é…ç½®
- **GPU**: NVIDIA RTX 3090 (24GB VRAM)
- **CPU**: (å¾…ç¡®è®¤)
- **RAM**: (å¾…ç¡®è®¤)
- **é©±åŠ¨**: NVIDIA Driver 570.124.04
- **CUDA**: 12.8
- **PyTorch**: 2.4.1

### æµ‹è¯•æ¨¡å‹
- **Model**: Qwen3-0.6B (æˆ–å…¶ä»–éœ€è¦æµ‹è¯•çš„æ¨¡å‹)
- **æ¨¡å‹å¤§å°**: (å¾…ç¡®è®¤)
- **ç²¾åº¦**: FP16

### å·¥ä½œè´Ÿè½½
- **å¹¶å‘åºåˆ—æ•°**: 256
- **è¾“å…¥é•¿åº¦**: 512 tokens
- **è¾“å‡ºé•¿åº¦**: 512 tokens
- **æ€»è¯·æ±‚æ•°**: æŒ‰éœ€ç¡®å®š

---

## ğŸ§ª å®Œæ•´æµ‹è¯•å¥—ä»¶

### ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€æ€§èƒ½æµ‹è¯•

#### æµ‹è¯• 1.1: ååé‡æµ‹è¯• (Throughput Benchmark)

**æµ‹è¯•ç›®æ ‡**: æµ‹é‡å•ä½æ—¶é—´å†…å¤„ç†çš„ tokens æ•°

**æµ‹è¯•æ–¹æ³•**:
```python
def test_throughput():
    """
    æµ‹è¯•æµç¨‹:
    1. é¢„çƒ­: è¿è¡Œ 1 ä¸ªå°æ‰¹æ¬¡ä½¿ GPU é¢„çƒ­
    2. è®¡æ—¶: ä»ç¬¬ä¸€ä¸ªè¯·æ±‚å¼€å§‹è®¡æ—¶
    3. å¤„ç†: å¤„ç† 256 ä¸ªåºåˆ—
    4. è®¡ç®—: tokens/æ—¶é—´ = ååé‡
    """
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.perf_counter()

    # å¤„ç†æ‰€æœ‰åºåˆ—
    for seq in sequences:
        output = model.generate(seq)

    # è®°å½•ç»“æŸæ—¶é—´
    end_time = time.perf_counter()

    # è®¡ç®—
    total_tokens = sum(len(seq) + output_len for seq in sequences)
    throughput = total_tokens / (end_time - start_time)

    return throughput  # tokens/s
```

**è¾“å‡ºæ•°æ®**:
- `prefill_throughput` (tokens/s)
- `decode_throughput` (tokens/s)
- `end_to_end_throughput` (tokens/s)
- `æ€»è€—æ—¶` (seconds)

**å¦‚ä½•éªŒè¯**: è¿è¡Œå¤šæ¬¡ï¼Œç»“æœåº”åœ¨ Â±5% èŒƒå›´å†…

---

#### æµ‹è¯• 1.2: å»¶è¿Ÿæµ‹è¯• (Latency Benchmark)

**æµ‹è¯•ç›®æ ‡**: æµ‹é‡ä»è¾“å…¥åˆ°ç¬¬ä¸€ä¸ªè¾“å‡º token çš„æ—¶é—´

**æµ‹è¯•æ–¹æ³•**:
```python
def test_latency():
    """
    æµ‹è¯•æµç¨‹:
    1. è¾“å…¥æç¤ºè¯ (512 tokens)
    2. è®°å½•ä»è¾“å…¥åˆ°ç¬¬ä¸€ä¸ª token çš„æ—¶é—´ (TTFT)
    3. è®°å½•åç»­æ¯ä¸ª token çš„å¹³å‡ç”Ÿæˆæ—¶é—´ (TPOT)
    """

    # é¦– Token å»¶è¿Ÿ (Time To First Token)
    start = time.perf_counter()
    first_token = model.generate_one_token(prompt)
    ttft = (time.perf_counter() - start) * 1000  # ms

    # å¹³å‡ Token æ—¶é—´ (Time Per Output Token)
    start = time.perf_counter()
    for i in range(128):  # ç”Ÿæˆ 128 ä¸ª tokens
        token = model.generate_one_token(...)
    tpot = (time.perf_counter() - start) * 1000 / 128  # ms/token

    return ttft, tpot
```

**è¾“å‡ºæ•°æ®**:
- `first_token_latency_ms`
- `time_per_output_token_ms`
- `total_latency_ms` (è®¡ç®—: TTFT + TPOT * output_length)

---

#### æµ‹è¯• 1.3: å†…å­˜ä½¿ç”¨æµ‹è¯• (Memory Profiling)

**æµ‹è¯•ç›®æ ‡**: æµ‹é‡å„ä¸ªç»„ä»¶çš„å†…å­˜å ç”¨

**æµ‹è¯•æ–¹æ³•**:
```python
def test_memory():
    """
    æµ‹è¯•æµç¨‹:
    1. åŠ è½½æ¨¡å‹å‰: è®°å½•åŸºçº¿å†…å­˜
    2. åŠ è½½æ¨¡å‹å: è®°å½•æ¨¡å‹æƒé‡å†…å­˜
    3. å¤„ç†åºåˆ—å: è®°å½• KV ç¼“å­˜å†…å­˜
    4. å¤„ç†è¿‡ç¨‹ä¸­: è®°å½•æœ€å¤§æ¿€æ´»å€¼å†…å­˜
    """

    import torch

    # åŸºçº¿å†…å­˜
    baseline = torch.cuda.memory_allocated()

    # åŠ è½½æ¨¡å‹
    model = load_model()
    model_memory = torch.cuda.memory_allocated() - baseline

    # å¤„ç†åºåˆ—
    torch.cuda.reset_peak_memory_stats()
    output = model.generate(sequences)

    # KV ç¼“å­˜å†…å­˜ (å³°å€¼ - æ¨¡å‹)
    peak_memory = torch.cuda.max_memory_allocated()
    kv_cache_memory = peak_memory - model_memory - baseline

    return model_memory, kv_cache_memory, peak_memory
```

**è¾“å‡ºæ•°æ®**:
- `model_weight_memory_gb`
- `kv_cache_memory_gb`
- `activation_memory_gb`
- `total_peak_memory_gb`

---

### ç¬¬äºŒéƒ¨åˆ†ï¼šä¼˜åŒ–éªŒè¯æµ‹è¯•

#### æµ‹è¯• 2.1: å‰ç¼€ç¼“å­˜æ”¶ç›Šæµ‹è¯•

**æµ‹è¯•ç›®æ ‡**: éªŒè¯å‰ç¼€ç¼“å­˜èƒ½èŠ‚çœå¤šå°‘è®¡ç®—

**æµ‹è¯•æ–¹æ³•**:
```python
def test_prefix_cache_benefit():
    """
    æµ‹è¯•æµç¨‹:
    1. å‡†å¤‡ 100 ä¸ªè¯·æ±‚ï¼Œå…¶ä¸­ 70% å…±äº«å‰ç¼€
    2. ç¦ç”¨ç¼“å­˜ï¼Œæµ‹é‡æ€§èƒ½ A
    3. å¯ç”¨ç¼“å­˜ï¼Œæµ‹è¯•æ€§èƒ½ B
    4. è®¡ç®—æ”¹è¿›æ¯”ä¾‹
    """

    # ç¦ç”¨ç¼“å­˜
    model.cache_enabled = False
    start = time.perf_counter()
    output_no_cache = model.generate_batch(requests_100)
    time_no_cache = time.perf_counter() - start
    tokens_no_cache = sum(len(req) + output_len for req in requests_100)

    # å¯ç”¨ç¼“å­˜
    model.cache_enabled = True
    start = time.perf_counter()
    output_with_cache = model.generate_batch(requests_100)
    time_with_cache = time.perf_counter() - start
    tokens_with_cache = sum(len(req) + output_len for req in requests_100)

    # è®¡ç®—èŠ‚çœ
    compute_saved = (tokens_no_cache - tokens_with_cache) / tokens_no_cache * 100
    cache_hit_rate = (tokens_no_cache - tokens_with_cache) / tokens_no_cache * 100
    speedup = time_no_cache / time_with_cache

    return compute_saved, cache_hit_rate, speedup
```

**è¾“å‡ºæ•°æ®**:
- `cache_hit_rate` (%)
- `compute_saved_pct` (%)
- `actual_speedup_ratio` (å€æ•°)

**é‡è¦**: è¿™ä¸ªæµ‹è¯•éœ€è¦**ä¸¤æ¬¡è¿è¡Œ**ï¼Œç»“æœåº”è¯¥èƒ½**ç›¸äº’éªŒè¯**

---

#### æµ‹è¯• 2.2: è°ƒåº¦å™¨æ•ˆç‡æµ‹è¯•

**æµ‹è¯•ç›®æ ‡**: éªŒè¯ Prefill/Decode åˆ†ç¦»è°ƒåº¦çš„æ•ˆæœ

**æµ‹è¯•æ–¹æ³•**:
```python
def test_scheduler_efficiency():
    """
    æµ‹è¯•æµç¨‹:
    1. è¿è¡Œæ··åˆè°ƒåº¦ (vLLM æ–¹å¼)
    2. è¿è¡Œåˆ†ç¦»è°ƒåº¦ (Nano-vLLM æ–¹å¼)
    3. æ¯”è¾ƒååé‡å’Œå»¶è¿Ÿ
    """

    # æ··åˆè°ƒåº¦
    mixed_scheduler = MixedScheduler()
    mixed_start = time.perf_counter()
    mixed_output = mixed_scheduler.generate(requests)
    mixed_time = time.perf_counter() - mixed_start

    # åˆ†ç¦»è°ƒåº¦
    separated_scheduler = SeparatedScheduler()
    sep_start = time.perf_counter()
    sep_output = separated_scheduler.generate(requests)
    sep_time = time.perf_counter() - sep_start

    improvement = (mixed_time - sep_time) / mixed_time * 100

    return improvement, mixed_time, sep_time
```

**è¾“å‡ºæ•°æ®**:
- `prefill_throughput`
- `decode_throughput`
- `improvement_vs_mixed_scheduler` (%)

---

#### æµ‹è¯• 2.3: CUDA å›¾æ•è·æ•ˆç›Šæµ‹è¯•

**æµ‹è¯•ç›®æ ‡**: éªŒè¯ CUDA å›¾æ•è·èƒ½èŠ‚çœå¤šå°‘ CPU å¼€é”€

**æµ‹è¯•æ–¹æ³•**:
```python
def test_cuda_graph_benefit():
    """
    æµ‹è¯•æµç¨‹:
    1. ç¦ç”¨ CUDA å›¾ï¼Œæµ‹é‡ decode é˜¶æ®µæ€§èƒ½ A
    2. å¯ç”¨ CUDA å›¾ï¼Œæµ‹é‡ decode é˜¶æ®µæ€§èƒ½ B
    3. è®¡ç®—æ”¹è¿›
    """

    # ç¦ç”¨ CUDA å›¾
    model.use_cuda_graph = False
    start = time.perf_counter()
    for i in range(decode_iterations):
        token = model.decode_step()
    time_no_graph = time.perf_counter() - start

    # å¯ç”¨ CUDA å›¾
    model.use_cuda_graph = True
    start = time.perf_counter()
    for i in range(decode_iterations):
        token = model.decode_step()
    time_with_graph = time.perf_counter() - start

    improvement = (time_no_graph - time_with_graph) / time_no_graph * 100

    return improvement, time_no_graph, time_with_graph
```

**è¾“å‡ºæ•°æ®**:
- `cpu_overhead_reduction` (%)
- `throughput_improvement` (%)

---

### ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¹¶è¡Œæ‰©å±•æ€§æµ‹è¯•

#### æµ‹è¯• 3.1: å•å¡åŸºå‡†

**æµ‹è¯•ç›®æ ‡**: å»ºç«‹ 1 å¡çš„æ€§èƒ½åŸºçº¿

**æµ‹è¯•æ–¹æ³•**:
```python
# ä»…ä½¿ç”¨ GPU 0
torch.cuda.set_device(0)
throughput_1gpu = run_benchmark()
```

---

#### æµ‹è¯• 3.2: å¤šå¡å¼ºç¼©æ”¾

**æµ‹è¯•ç›®æ ‡**: å›ºå®šé—®é¢˜è§„æ¨¡ï¼Œå¢åŠ  GPU

**æµ‹è¯•æ–¹æ³•**:
```python
def test_strong_scaling():
    """
    å¼ºç¼©æ”¾: é—®é¢˜è§„æ¨¡å›ºå®šï¼Œå¢åŠ  GPU æ•°

    ç†æƒ³æƒ…å†µ: N ä¸ª GPU åº”è¯¥æœ‰ N å€çš„ååé‡
    å®é™…æƒ…å†µ: é€šä¿¡å¼€é”€ä¼šé™ä½æ•ˆç‡
    """

    baseline_throughput = throughput_1gpu  # 1362.13 tokens/s (ä» 1 GPU)

    for num_gpus in [1, 2, 4, 8]:
        throughput = run_benchmark_with_n_gpus(num_gpus)
        efficiency = throughput / (baseline_throughput * num_gpus) * 100
        # è®°å½•: throughput, efficiency
```

**è¾“å‡ºæ•°æ®**:
- `1gpu_throughput`
- `2gpu_throughput` å’Œ `2gpu_efficiency`
- `4gpu_throughput` å’Œ `4gpu_efficiency`
- `8gpu_throughput` å’Œ `8gpu_efficiency`

---

#### æµ‹è¯• 3.3: å¤šå¡å¼±ç¼©æ”¾

**æµ‹è¯•ç›®æ ‡**: æ¯ GPU å·¥ä½œé‡å›ºå®šï¼Œå¢åŠ  GPU

**æµ‹è¯•æ–¹æ³•**:
```python
def test_weak_scaling():
    """
    å¼±ç¼©æ”¾: æ¯ä¸ª GPU çš„å·¥ä½œé‡å›ºå®šï¼Œå¢åŠ  GPU æ•°

    ç†æƒ³æƒ…å†µ: æ€»ååé‡åº”è¯¥æ¥è¿‘çº¿æ€§å¢é•¿
    """

    for num_gpus in [1, 2, 4, 8]:
        # æ¯ä¸ª GPU å¤„ç†ç›¸åŒæ•°é‡çš„åºåˆ—
        sequences_per_gpu = 32
        total_sequences = sequences_per_gpu * num_gpus

        throughput = run_benchmark_with_n_gpus(num_gpus, total_sequences)
        # è®°å½•: throughput (åº”è¯¥æ¥è¿‘çº¿æ€§å¢é•¿)
```

**è¾“å‡ºæ•°æ®**:
- `1gpu_throughput`
- `2gpu_throughput`
- `4gpu_throughput`
- `8gpu_throughput`

---

### ç¬¬å››éƒ¨åˆ†ï¼šä»£ç è´¨é‡æµ‹è¯•

#### æµ‹è¯• 4.1: ä»£ç è¡Œæ•°ç»Ÿè®¡

**æµ‹è¯•æ–¹æ³•**:
```python
def analyze_code():
    """
    ç»Ÿè®¡ä»£ç è´¨é‡æŒ‡æ ‡
    """
    total_lines = 0
    total_functions = 0
    total_classes = 0

    for python_file in all_python_files:
        with open(python_file) as f:
            content = f.read()
            lines = len(content.split('\n'))
            functions = content.count('def ')
            classes = content.count('class ')

            total_lines += lines
            total_functions += functions
            total_classes += classes

    return {
        'total_lines': total_lines,
        'total_functions': total_functions,
        'total_classes': total_classes,
        'avg_lines_per_function': total_lines / total_functions,
        'cyclomatic_complexity': calculate_complexity()
    }
```

**è¾“å‡ºæ•°æ®**:
- `total_loc`
- `function_count`
- `class_count`
- `avg_complexity`

---

## ğŸ“Š æœŸæœ›çš„è¾“å‡ºæ ¼å¼

æ¯ä¸ªæµ‹è¯•åº”è¯¥ç”Ÿæˆï¼š

```json
{
  "test_name": "ååé‡æµ‹è¯•",
  "test_date": "2026-02-19",
  "hardware": "RTX 3090",
  "method": "å¤„ç† 256 ä¸ªåºåˆ—ï¼Œæ¯ä¸ª 512+512 tokens",
  "parameters": {
    "batch_size": 256,
    "input_length": 512,
    "output_length": 512
  },
  "results": {
    "prefill_throughput": 395297.08,  // tokens/s
    "decode_throughput": 131072000.0,  // tokens/s
    "e2e_throughput": 790444.13,  // tokens/s
    "total_time": 0.3316  // seconds
  },
  "how_obtained": "ä½¿ç”¨ torch.cuda.synchronize() ç²¾ç¡®è®¡æ—¶ï¼Œå¤„ç†å®Œæ•´çš„ç”Ÿæˆæµç¨‹",
  "verification": "è¿è¡Œ 3 æ¬¡ï¼Œç»“æœåœ¨ Â±5% èŒƒå›´å†…"
}
```

---

## âœ… æµ‹è¯•æ£€æŸ¥æ¸…å•

- [ ] æ‰€æœ‰æµ‹è¯•ä½¿ç”¨ç›¸åŒç¡¬ä»¶ (RTX 3090)
- [ ] æ¯ä¸ªæ•°æ®éƒ½è®°å½•äº†è·å–æ–¹æ³•
- [ ] æ¯ä¸ªæ•°æ®éƒ½å¯ä»¥ç‹¬ç«‹éªŒè¯
- [ ] æ²¡æœ‰å¼•ç”¨å¤–éƒ¨æ•°æ®ï¼ˆå¦‚ README ä¸­çš„æ•°æ®ï¼‰
- [ ] ç¦ç”¨/å¯ç”¨åŠŸèƒ½çš„å¯¹æ¯”æµ‹è¯•éƒ½æˆå¯¹å‡ºç°
- [ ] æ‰€æœ‰è®¡æ—¶éƒ½ä½¿ç”¨ `torch.cuda.synchronize()`
- [ ] å…³é”®æµ‹è¯•è¿è¡Œå¤šæ¬¡éªŒè¯ç¨³å®šæ€§

---

## ğŸ¯ æœ€ç»ˆè¾“å‡º

æ‰€æœ‰æµ‹è¯•å®Œæˆåï¼Œç”Ÿæˆä¸€ä»½æ–‡æ¡£ï¼Œæ¸…æ¥šåœ°è¯´æ˜ï¼š

```markdown
# Nano-vLLM æ€§èƒ½è¯„ä¼° - å®Œæ•´æµ‹è¯•æŠ¥å‘Š

## 1. æ¨ç†ååé‡

**æ•°æ®**: 1434.13 tokens/s

**è·å–æ–¹æ³•**:
- å¤„ç† 256 ä¸ªå¹¶å‘åºåˆ—
- æ¯ä¸ªåºåˆ— 512 token è¾“å…¥ + 512 token è¾“å‡º
- ä½¿ç”¨ torch.cuda.synchronize() ç²¾ç¡®è®¡æ—¶
- æ€»å…±å¤„ç† 262,144 tokensï¼Œè€—æ—¶ 0.3316 ç§’
- è®¡ç®—: 262,144 tokens / 0.3316 s = 790,444 tokens/s (ç«¯åˆ°ç«¯)

**éªŒè¯æ–¹æ³•**:
- è¿è¡Œ 3 æ¬¡ï¼Œç»“æœåˆ†åˆ«ä¸º: xxx, xxx, xxx
- æ³¢åŠ¨èŒƒå›´: Â±3% (å¯æ¥å—)

## 2. å‰ç¼€ç¼“å­˜æ”¶ç›Š

**æ•°æ®**: 55.92% è®¡ç®—èŠ‚çœï¼Œç¼“å­˜å‘½ä¸­ç‡ 55.92%

**è·å–æ–¹æ³•**:
- å‡†å¤‡ 100 ä¸ªè¯·æ±‚
- å…¶ä¸­ 70% è¯·æ±‚å…±äº« 80% çš„å‰ç¼€
- ç¦ç”¨ç¼“å­˜: å¤„ç† 51,200 tokens
- å¯ç”¨ç¼“å­˜: å¤„ç† 22,570 tokens
- è®¡ç®—: (51,200 - 22,570) / 51,200 = 55.92%

... ä»¥æ­¤ç±»æ¨
```

---

è¿™å°±æ˜¯æˆ‘çš„å®Œæ•´æµ‹è¯•è®¡åˆ’ã€‚æ‰€æœ‰æ•°æ®éƒ½åº”è¯¥æ¥è‡ªè¿™å¥—å®Œæ•´çš„æµ‹è¯•æ¡†æ¶ã€‚

ä½ è®¤ä¸ºè¿™ä¸ªè®¡åˆ’åˆç†å—ï¼Ÿè¿˜æ˜¯éœ€è¦è°ƒæ•´æŸäº›æµ‹è¯•æ–¹æ³•ï¼Ÿ
