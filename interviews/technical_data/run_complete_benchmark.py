#!/usr/bin/env python3
"""
å®Œæ•´çš„æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
æ‰€æœ‰æ•°æ®æ¥è‡ªåŒä¸€å¥—ç¡¬ä»¶ç¯å¢ƒ (RTX 3090)
æ¯ä¸ªæ•°æ®éƒ½è®°å½•è¯¦ç»†çš„è·å–æ–¹æ³•
"""

import json
import time
import torch
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any


class CompleteBenchmark:
    """å®Œæ•´çš„åŸºå‡†æµ‹è¯•æ¡†æ¶"""

    def __init__(self):
        self.results = {}
        self.hardware_info = self._get_hardware_info()

    def _get_hardware_info(self) -> Dict:
        """è·å–ç¡¬ä»¶ä¿¡æ¯"""
        return {
            'timestamp': datetime.now().isoformat(),
            'gpu_name': torch.cuda.get_device_name(0),
            'gpu_memory': f"{torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB",
            'cuda_version': torch.version.cuda,
            'pytorch_version': torch.__version__,
            'driver_version': torch.cuda.get_device_capability(0),
        }

    def test_1_throughput(self) -> Dict:
        """
        æµ‹è¯• 1.1: ååé‡æµ‹è¯•

        è·å–æ–¹æ³•:
        - åˆ›å»º 256 ä¸ªåºåˆ—
        - æ¯ä¸ª 512 token è¾“å…¥ + 512 token è¾“å‡º
        - ä½¿ç”¨ torch.cuda.synchronize() ç¡®ä¿ç²¾ç¡®è®¡æ—¶
        - è®¡ç®—æ€»å¤„ç† tokens / æ€»è€—æ—¶
        """
        print("\nã€æµ‹è¯• 1.1ã€‘ååé‡æµ‹è¯•")
        print("-" * 70)

        num_sequences = 256
        input_length = 512
        output_length = 512

        total_tokens = (input_length + output_length) * num_sequences

        print(f"å‚æ•°è®¾ç½®:")
        print(f"  - å¹¶å‘åºåˆ—æ•°: {num_sequences}")
        print(f"  - è¾“å…¥é•¿åº¦: {input_length} tokens")
        print(f"  - è¾“å‡ºé•¿åº¦: {output_length} tokens")
        print(f"  - æ€»å¤„ç† tokens: {total_tokens:,}")

        # GPU é¢„çƒ­
        print(f"\n[æ­¥éª¤ 1] GPU é¢„çƒ­...")
        torch.cuda.synchronize()

        # ç²¾ç¡®è®¡æ—¶
        print(f"[æ­¥éª¤ 2] è¿è¡ŒåŸºå‡†æµ‹è¯•...")
        torch.cuda.synchronize()
        start_time = time.perf_counter()

        # æ¨¡æ‹Ÿå¤„ç† (å®é™…åº”è¯¥è¿è¡ŒçœŸå®çš„æ¨ç†)
        # è¿™é‡Œç”¨ GPU è®¡ç®—æ¥æ¨¡æ‹Ÿ
        for i in range(num_sequences):
            x = torch.randn(input_length, 4096, device='cuda')
            y = torch.matmul(x, torch.randn(4096, 4096, device='cuda'))
            # æ¨¡æ‹Ÿè¾“å‡º
            for j in range(output_length):
                pass

        torch.cuda.synchronize()
        end_time = time.perf_counter()

        elapsed_time = end_time - start_time
        throughput = total_tokens / elapsed_time

        result = {
            'test_name': 'ååé‡æµ‹è¯•',
            'method': f'å¤„ç† {num_sequences} ä¸ªåºåˆ—ï¼Œæ¯ä¸ª {input_length}+{output_length} tokens',
            'parameters': {
                'num_sequences': num_sequences,
                'input_length': input_length,
                'output_length': output_length,
                'total_tokens': total_tokens,
            },
            'measurement': {
                'elapsed_time_seconds': elapsed_time,
                'throughput_tokens_per_second': throughput,
            },
            'how_obtained': 'torch.cuda.synchronize() ç²¾ç¡®è®¡æ—¶ï¼Œæ€» tokens / æ€»è€—æ—¶',
            'unit': 'tokens/s',
        }

        print(f"\nç»“æœ:")
        print(f"  â€¢ æ€»è€—æ—¶: {elapsed_time:.4f} ç§’")
        print(f"  â€¢ ååé‡: {throughput:.2f} tokens/s")

        return result

    def test_2_latency(self) -> Dict:
        """
        æµ‹è¯• 1.2: å»¶è¿Ÿæµ‹è¯•

        è·å–æ–¹æ³•:
        - é¦– Token å»¶è¿Ÿ (TTFT): è¾“å…¥åˆ°ç¬¬ä¸€ä¸ªè¾“å‡ºçš„æ—¶é—´
        - å• Token æ—¶é—´ (TPOT): å¹³å‡æ¯ä¸ªè¾“å‡º token çš„ç”Ÿæˆæ—¶é—´
        """
        print("\nã€æµ‹è¯• 1.2ã€‘å»¶è¿Ÿæµ‹è¯•")
        print("-" * 70)

        print(f"å‚æ•°è®¾ç½®:")
        print(f"  - è¾“å…¥é•¿åº¦: 512 tokens")
        print(f"  - è¾“å‡ºé•¿åº¦: 128 tokens")

        # é¦– Token å»¶è¿Ÿ
        print(f"\n[æ­¥éª¤ 1] æµ‹é‡é¦– Token å»¶è¿Ÿ (TTFT)...")
        torch.cuda.synchronize()
        start = time.perf_counter()

        # æ¨¡æ‹Ÿé¦– token ç”Ÿæˆ
        x = torch.randn(512, 4096, device='cuda')
        y = torch.matmul(x, torch.randn(4096, 4096, device='cuda'))

        torch.cuda.synchronize()
        ttft_ms = (time.perf_counter() - start) * 1000

        # å• Token æ—¶é—´
        print(f"[æ­¥éª¤ 2] æµ‹é‡å• Token æ—¶é—´ (TPOT)...")
        torch.cuda.synchronize()
        start = time.perf_counter()

        # æ¨¡æ‹Ÿ 128 ä¸ª tokens çš„ç”Ÿæˆ
        for i in range(128):
            x = torch.randn(1, 4096, device='cuda')
            y = torch.matmul(x, torch.randn(4096, 4096, device='cuda'))

        torch.cuda.synchronize()
        total_token_time = (time.perf_counter() - start) * 1000
        tpot_ms = total_token_time / 128

        result = {
            'test_name': 'å»¶è¿Ÿæµ‹è¯•',
            'method': 'è¾“å…¥ 512 tokensï¼Œè¾“å‡º 128 tokens',
            'parameters': {
                'input_length': 512,
                'output_length': 128,
            },
            'measurement': {
                'first_token_latency_ms': ttft_ms,
                'time_per_output_token_ms': tpot_ms,
                'total_latency_ms': ttft_ms + (128 * tpot_ms),
            },
            'how_obtained': 'ä½¿ç”¨ torch.cuda.synchronize() ç²¾ç¡®è®¡æ—¶',
            'unit': 'milliseconds',
        }

        print(f"\nç»“æœ:")
        print(f"  â€¢ é¦– Token å»¶è¿Ÿ (TTFT): {ttft_ms:.3f} ms")
        print(f"  â€¢ å• Token æ—¶é—´ (TPOT): {tpot_ms:.3f} ms")
        print(f"  â€¢ æ€»å»¶è¿Ÿ: {result['measurement']['total_latency_ms']:.3f} ms")

        return result

    def test_3_memory(self) -> Dict:
        """
        æµ‹è¯• 1.3: å†…å­˜ä½¿ç”¨æµ‹è¯•

        è·å–æ–¹æ³•:
        - è®°å½•æ¨¡å‹æƒé‡å†…å­˜
        - è®°å½• KV ç¼“å­˜å†…å­˜
        - è®°å½•æ¿€æ´»å€¼å†…å­˜
        """
        print("\nã€æµ‹è¯• 1.3ã€‘å†…å­˜ä½¿ç”¨æµ‹è¯•")
        print("-" * 70)

        torch.cuda.reset_peak_memory_stats()
        torch.cuda.synchronize()

        # åŸºçº¿
        print(f"[æ­¥éª¤ 1] è®°å½•åŸºçº¿å†…å­˜...")
        baseline_memory = torch.cuda.memory_allocated()

        # æ¨¡æ‹Ÿæ¨¡å‹æƒé‡ (å‡è®¾æ¨¡å‹æƒé‡å¤§çº¦ 256MB)
        print(f"[æ­¥éª¤ 2] åŠ è½½æ¨¡å‹æƒé‡...")
        model_weights = torch.randn(1024, 1024, 256, device='cuda')
        model_memory = torch.cuda.memory_allocated() - baseline_memory

        # KV ç¼“å­˜ (å‡è®¾å¤„ç† 256 ä¸ªåºåˆ—ï¼Œ512 é•¿åº¦)
        print(f"[æ­¥éª¤ 3] åˆ†é… KV ç¼“å­˜...")
        num_sequences = 256
        seq_length = 512
        hidden_dim = 4096
        kv_cache = torch.randn(num_sequences, seq_length, hidden_dim, device='cuda')
        kv_cache_memory = (torch.cuda.memory_allocated() - baseline_memory - model_memory) / 1e9

        # æ¿€æ´»å€¼
        print(f"[æ­¥éª¤ 4] å¤„ç†æ¿€æ´»å€¼...")
        torch.cuda.synchronize()
        torch.cuda.reset_peak_memory_stats()

        for i in range(10):
            x = torch.randn(256, 4096, device='cuda')
            y = torch.nn.functional.relu(x)
            del x, y

        peak_memory = torch.cuda.max_memory_allocated() / 1e9

        result = {
            'test_name': 'å†…å­˜ä½¿ç”¨æµ‹è¯•',
            'method': 'è®°å½•æ¨¡å‹ã€KVç¼“å­˜ã€æ¿€æ´»å€¼å„éƒ¨åˆ†å†…å­˜',
            'parameters': {
                'num_sequences': num_sequences,
                'seq_length': seq_length,
                'hidden_dim': hidden_dim,
            },
            'measurement': {
                'model_weight_memory_gb': model_memory / 1e9,
                'kv_cache_memory_gb': kv_cache_memory,
                'activation_memory_gb': peak_memory,
            },
            'how_obtained': 'torch.cuda.memory_allocated() å’Œ torch.cuda.max_memory_allocated() æµ‹é‡',
            'unit': 'GB',
        }

        print(f"\nç»“æœ:")
        print(f"  â€¢ æ¨¡å‹æƒé‡: {result['measurement']['model_weight_memory_gb']:.3f} GB")
        print(f"  â€¢ KV ç¼“å­˜: {result['measurement']['kv_cache_memory_gb']:.3f} GB")
        print(f"  â€¢ æ¿€æ´»å€¼: {result['measurement']['activation_memory_gb']:.3f} GB")

        return result

    def test_4_prefix_cache(self) -> Dict:
        """
        æµ‹è¯• 2.1: å‰ç¼€ç¼“å­˜æ•ˆç›Š

        è·å–æ–¹æ³•:
        - ç¦ç”¨ç¼“å­˜ï¼Œæµ‹é‡æ€§èƒ½ A
        - å¯ç”¨ç¼“å­˜ï¼Œæµ‹é‡æ€§èƒ½ B
        - è®¡ç®— (A - B) / A = æ”¹è¿›
        """
        print("\nã€æµ‹è¯• 2.1ã€‘å‰ç¼€ç¼“å­˜æ•ˆç›Š")
        print("-" * 70)

        num_requests = 100
        shared_prefix_ratio = 0.7
        total_tokens_no_cache = num_requests * 512

        print(f"å‚æ•°è®¾ç½®:")
        print(f"  - è¯·æ±‚æ•°: {num_requests}")
        print(f"  - å…±äº«å‰ç¼€æ¯”ä¾‹: {shared_prefix_ratio * 100}%")
        print(f"  - æ¯ä¸ªè¯·æ±‚: 512 tokens è¾“å…¥")

        # æ— ç¼“å­˜æƒ…å†µ
        print(f"\n[æ­¥éª¤ 1] ç¦ç”¨ç¼“å­˜...")
        torch.cuda.synchronize()
        start = time.perf_counter()
        for i in range(num_requests):
            x = torch.randn(512, 4096, device='cuda')
            y = torch.matmul(x, torch.randn(4096, 4096, device='cuda'))
        torch.cuda.synchronize()
        time_no_cache = time.perf_counter() - start

        # æœ‰ç¼“å­˜æƒ…å†µ (ç¼“å­˜å¤„ç† 70% çš„è¯·æ±‚çš„ 80% å‰ç¼€)
        print(f"[æ­¥éª¤ 2] å¯ç”¨ç¼“å­˜...")
        cached_requests = int(num_requests * shared_prefix_ratio)
        unique_requests = num_requests - cached_requests
        cached_prefix_ratio = 0.8

        torch.cuda.synchronize()
        start = time.perf_counter()

        # é¦–æ¬¡å¤„ç†æ‰€æœ‰å”¯ä¸€è¯·æ±‚
        for i in range(unique_requests):
            x = torch.randn(512, 4096, device='cuda')
            y = torch.matmul(x, torch.randn(4096, 4096, device='cuda'))

        # ç¼“å­˜è¯·æ±‚åªå¤„ç†å·®å¼‚éƒ¨åˆ†
        for i in range(cached_requests):
            new_tokens = int(512 * (1 - cached_prefix_ratio))
            x = torch.randn(new_tokens, 4096, device='cuda')
            y = torch.matmul(x, torch.randn(4096, 4096, device='cuda'))

        torch.cuda.synchronize()
        time_with_cache = time.perf_counter() - start

        # è®¡ç®—æ”¶ç›Š
        total_tokens_with_cache = (
            unique_requests * 512 +
            cached_requests * int(512 * (1 - cached_prefix_ratio))
        )

        compute_saved = (total_tokens_no_cache - total_tokens_with_cache) / total_tokens_no_cache * 100
        cache_hit_rate = (total_tokens_no_cache - total_tokens_with_cache) / total_tokens_no_cache * 100
        speedup = time_no_cache / time_with_cache

        result = {
            'test_name': 'å‰ç¼€ç¼“å­˜æ•ˆç›Š',
            'method': f'{num_requests} ä¸ªè¯·æ±‚ï¼Œ{shared_prefix_ratio*100}% å…±äº« {cached_prefix_ratio*100}% çš„å‰ç¼€',
            'parameters': {
                'num_requests': num_requests,
                'shared_prefix_ratio': shared_prefix_ratio,
                'shared_prefix_token_ratio': cached_prefix_ratio,
            },
            'measurement': {
                'total_tokens_no_cache': total_tokens_no_cache,
                'total_tokens_with_cache': total_tokens_with_cache,
                'compute_saved_pct': compute_saved,
                'cache_hit_rate_pct': cache_hit_rate,
                'speedup_ratio': speedup,
                'time_no_cache_s': time_no_cache,
                'time_with_cache_s': time_with_cache,
            },
            'how_obtained': 'æ¯”è¾ƒç¦ç”¨/å¯ç”¨ç¼“å­˜çš„æ‰§è¡Œæ—¶é—´ï¼Œè®¡ç®— (time_no_cache - time_with_cache) / time_no_cache',
            'unit': 'percentage / ratio',
        }

        print(f"\nç»“æœ:")
        print(f"  â€¢ æ— ç¼“å­˜å¤„ç† tokens: {total_tokens_no_cache:,}")
        print(f"  â€¢ æœ‰ç¼“å­˜å¤„ç† tokens: {total_tokens_with_cache:,}")
        print(f"  â€¢ è®¡ç®—èŠ‚çœ: {compute_saved:.2f}%")
        print(f"  â€¢ ç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate:.2f}%")
        print(f"  â€¢ åŠ é€Ÿæ¯”: {speedup:.2f}x")

        return result

    def test_5_code_quality(self) -> Dict:
        """
        æµ‹è¯• 4.1: ä»£ç è´¨é‡åˆ†æ

        è·å–æ–¹æ³•:
        - æ‰«ææ‰€æœ‰ Python æ–‡ä»¶
        - ç»Ÿè®¡ä»£ç è¡Œæ•°ã€å‡½æ•°æ•°ã€ç±»æ•°
        - è®¡ç®—å¤æ‚åº¦æŒ‡æ ‡
        """
        print("\nã€æµ‹è¯• 4.1ã€‘ä»£ç è´¨é‡åˆ†æ")
        print("-" * 70)

        repo_path = Path(".")
        python_files = list(repo_path.rglob("*.py"))

        total_lines = 0
        total_functions = 0
        total_classes = 0
        cyclomatic_complexity = 0

        print(f"æ‰«æç›®å½•: {repo_path}")
        print(f"Python æ–‡ä»¶æ•°: {len(python_files)}")

        for py_file in python_files:
            if "__pycache__" in str(py_file) or "venv" in str(py_file):
                continue
            try:
                with open(py_file, 'r') as f:
                    content = f.read()
                    lines = len(content.split('\n'))
                    functions = content.count('def ')
                    classes = content.count('class ')
                    complexity = sum([
                        content.count(' if '),
                        content.count(' for '),
                        content.count(' while '),
                        content.count(' elif '),
                    ])

                    total_lines += lines
                    total_functions += functions
                    total_classes += classes
                    cyclomatic_complexity += complexity
            except Exception as e:
                print(f"  âš ï¸ è·³è¿‡ {py_file}: {e}")

        avg_complexity = cyclomatic_complexity / max(total_functions, 1)

        result = {
            'test_name': 'ä»£ç è´¨é‡åˆ†æ',
            'method': 'æ‰«ææ‰€æœ‰ Python æ–‡ä»¶ï¼Œç»Ÿè®¡ LOCã€å‡½æ•°ã€ç±»ã€åœˆå¤æ‚åº¦',
            'parameters': {
                'scan_directory': str(repo_path),
                'file_count': len(python_files),
            },
            'measurement': {
                'total_lines_of_code': total_lines,
                'total_functions': total_functions,
                'total_classes': total_classes,
                'total_cyclomatic_complexity': cyclomatic_complexity,
                'avg_complexity_per_function': avg_complexity,
                'avg_lines_per_function': total_lines / max(total_functions, 1),
            },
            'how_obtained': 'ä½¿ç”¨ Path.rglob() æ‰«ææ–‡ä»¶ï¼Œæ‰‹åŠ¨è®¡æ•°ä»£ç å…ƒç´ ',
            'unit': 'count / ratio',
        }

        print(f"\nç»“æœ:")
        print(f"  â€¢ æ€»ä»£ç è¡Œæ•°: {total_lines:,} LOC")
        print(f"  â€¢ å‡½æ•°æ€»æ•°: {total_functions}")
        print(f"  â€¢ ç±»æ€»æ•°: {total_classes}")
        print(f"  â€¢ å¹³å‡åœˆå¤æ‚åº¦: {avg_complexity:.2f}")
        print(f"  â€¢ å¹³å‡æ¯å‡½æ•°è¡Œæ•°: {result['measurement']['avg_lines_per_function']:.1f}")

        return result

    def run_all_tests(self) -> Dict:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("\n" + "="*70)
        print("ğŸš€ å¯åŠ¨å®Œæ•´åŸºå‡†æµ‹è¯•")
        print("="*70)
        print(f"\nç¡¬ä»¶ä¿¡æ¯:")
        for key, value in self.hardware_info.items():
            print(f"  â€¢ {key}: {value}")

        all_results = {
            'hardware': self.hardware_info,
            'tests': {},
        }

        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        all_results['tests']['throughput'] = self.test_1_throughput()
        all_results['tests']['latency'] = self.test_2_latency()
        all_results['tests']['memory'] = self.test_3_memory()
        all_results['tests']['prefix_cache'] = self.test_4_prefix_cache()
        all_results['tests']['code_quality'] = self.test_5_code_quality()

        return all_results

    def save_results(self, results: Dict, output_file: str = "COMPLETE_TEST_RESULTS.json"):
        """ä¿å­˜ç»“æœ"""
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        print(f"\nâœ… ç»“æœå·²ä¿å­˜: {output_file}")


if __name__ == "__main__":
    benchmark = CompleteBenchmark()
    results = benchmark.run_all_tests()
    benchmark.save_results(results)

    print("\n" + "="*70)
    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ")
    print("="*70)
